{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的库\n",
    "import torch                       # PyTorch 主库\n",
    "import torch.nn.functional as F    # 常用的神经网络函数库（激活函数、loss 等）\n",
    "import matplotlib.pyplot as plt    # 画图用\n",
    "%matplotlib inline                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取所有名字数据，每一行是一个名字\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]  # 看前 8 个，确认读进来长什么样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)  # 一共有多少个名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# 构建字符表：字符到整数、整数到字符的双向映射\n",
    "chars = sorted(list(set(''.join(words))))     # 所有名字拼在一起，取去重后的所有字符\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}  # 每个字符分配一个从 1 开始的 id\n",
    "stoi['.'] = 0                                 # 特殊符号 '.' 用 0 表示（作为开始/结束符）\n",
    "itos = {i: s for s, i in stoi.items()}        # 反向映射：id -> 字符\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单版本：直接把所有 (context, next_char) 做成 X, Y\n",
    "# context：前 block_size 个字符（用整数 id 表示）\n",
    "# Y：下一个要预测的字符的 id\n",
    "\n",
    "block_size = 3  # 上下文长度：用 3 个字符预测下一个\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "  \n",
    "  # print(w)  # 如果想看过程，可以打开\n",
    "  context = [0] * block_size      # 用 0 作为起始上下文（相当于前面补 '...'）\n",
    "  for ch in w + '.':              # 最后再拼一个 '.' 作为结束符\n",
    "    ix = stoi[ch]                 # 当前字符的 id\n",
    "    X.append(context)             # 把当前的 3 个字符（context）存进 X\n",
    "    Y.append(ix)                  # 下一个要预测的字符 id 存进 Y\n",
    "    # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    # 上面这一行可以打印：当前上下文字符 -> 目标字符\n",
    "    context = context[1:] + [ix]  # 滑动窗口：丢掉最早的一个，加上当前的 id\n",
    "  \n",
    "X = torch.tensor(X)               # 转成张量，形状大概是 (样本数, 3)\n",
    "Y = torch.tensor(Y)               # 一维张量，长度和 X 行数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype  # 看一下形状和数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# 更“正规”的版本：封装成函数，并划分 train / dev / test\n",
    "block_size = 3  # 再次声明，方便下面引用\n",
    "\n",
    "def build_dataset(words):\n",
    "  \"\"\"\n",
    "  给定一组词（字符串列表），生成对应的 (X, Y) 数据集。\n",
    "  X: 每一行是一个长度为 block_size 的上下文（由整数 id 组成）\n",
    "  Y: 每一行是一个目标字符 id\n",
    "  \"\"\"\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    # print(w)\n",
    "    context = [0] * block_size          # 初始上下文全 0\n",
    "    for ch in w + '.':                  # 依次遍历名字里的字符，再加终止符 '.'\n",
    "      ix = stoi[ch]                     # 字符 -> id\n",
    "      X.append(context)                 # 当前上下文\n",
    "      Y.append(ix)                      # 目标字符\n",
    "      # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix]      # 滑动窗口向前移动一格\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)               # 打印一下这个子数据集的大小\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)                         # 固定随机种子，保证可复现\n",
    "random.shuffle(words)                   # 打乱所有名字的顺序\n",
    "\n",
    "n1 = int(0.8 * len(words))              # 前 80% 作为训练集\n",
    "n2 = int(0.9 * len(words))              # 接下来 10% 作为 dev，最后 10% 作为 test\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])    # 训练集\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])# 验证集\n",
    "Xte, Yte = build_dataset(words[n2:])    # 测试集\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建embedding嵌入矩阵 C：把离散的字符 id 映射到一个低维向量空间\n",
    "C = torch.randn((27, 2))  # 一共有 27 个符号，每个用 2 维向量表示（随机初始化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用嵌入矩阵查表：\n",
    "# X 是 (N, block_size) 的整数张量，C[X] 会得到 (N, block_size, 2) 的嵌入向量\n",
    "emb = C[X]\n",
    "emb.shape  # (样本数, 3, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义第一层（隐藏层）的权重和偏置\n",
    "# 输入维度 = 3（block_size）* 2（嵌入维度） = 6\n",
    "# 隐藏层维度 = 100\n",
    "W1 = torch.randn((6, 100))  # 权重矩阵\n",
    "b1 = torch.randn(100)       # 偏置向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 (N, 3, 2) 展平为 (N, 6)，然后做线性变换 + tanh 激活\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)  # h: (N, 100)，隐藏层输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3348, -0.5060,  0.1968,  ..., -0.9916, -0.9821,  0.8410],\n",
       "        [-0.4534, -0.4739,  0.9383,  ..., -0.9633,  0.1245,  0.9146],\n",
       "        [ 0.9494,  0.8611, -0.9748,  ..., -0.9998, -0.9998,  0.1556],\n",
       "        ...,\n",
       "        [-0.9973, -0.8212,  0.9888,  ...,  0.9542, -0.9993, -0.8426],\n",
       "        [-0.7395,  0.4131,  0.7871,  ...,  0.9734, -0.9940,  0.7995],\n",
       "        [-0.9153, -0.5571,  0.9944,  ...,  0.9863, -0.9909, -0.8887]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输出层参数：从 100 维隐藏层 -> 27 维输出（每个字符一个 logit）\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算输出 logits（未归一化的分数）\n",
    "logits = h @ W2 + b2  # 形状 (N, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手工计算 softmax（只做示意，后面会用 F.cross_entropy 更efficient）\n",
    "counts = logits.exp()                          # 先对每个 logit 做指数\n",
    "prob = counts / counts.sum(1, keepdims=True)   # 每一行除以这一行的和 -> 概率分布\n",
    "prob.shape                                     # (N, 27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单手写一个 loss（负对数似然），这里只是说明：\n",
    "#loss = -prob[torch.arange(32), Y].log().mean()  # 这里只用了前 32 个样本\n",
    "#loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ now made respectable :) ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正式开始用 train/dev/test + MLP + cross_entropy 训练\n",
    "\n",
    "Xtr.shape, Ytr.shape  # 看训练集尺寸（样本数，3），（样本数,）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用固定随机种子初始化所有参数\n",
    "g = torch.Generator().manual_seed(2147483647)   # 为 PyTorch 的随机数生成器设定种子\n",
    "C = torch.randn((27, 10),  generator=g)         # 嵌入维度改为 10\n",
    "W1 = torch.randn((30, 200), generator=g)        # 输入 3*10=30，隐藏层 200\n",
    "b1 = torch.randn(200,      generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)        # 从 200 -> 27\n",
    "b2 = torch.randn(27,        generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]                # 把所有参数放在一个列表里方便操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计一下总参数量\n",
    "sum(p.nelement() for p in parameters)  # 所有参数的元素个数总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开启所有参数的梯度跟踪\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选：准备一组学习率，用于搜索（这里只是先生成，下面注释掉了）\n",
    "lre = torch.linspace(-3, 0, 1000)  # 在 [10^-3, 10^0] 上取对数均匀的 1000 个点\n",
    "lrs = 10**lre                       # 实际的学习率数组（指数变换回来）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来记录训练过程的学习率、loss、步数\n",
    "lri = []       # 对应 lre 的索引（这里实际没用到）\n",
    "lossi = []     # 每一步的 loss（取 log10 方便画图）\n",
    "stepi = []     # 训练步数 i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环：在训练集上随机抽取 minibatch 做 SGD\n",
    "for i in range(200000):\n",
    "  \n",
    "  # minibatch 构造：随机取 32 个样本\n",
    "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xtr[ix]]                             # (32, 3, 10)，查嵌入\n",
    "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1)   # (32, 200) 隐藏层\n",
    "  logits = h @ W2 + b2                         # (32, 27) 输出层\n",
    "  loss = F.cross_entropy(logits, Ytr[ix])      # 直接用内置的 cross_entropy 计算 NLL loss\n",
    "  # print(loss.item())\n",
    "  \n",
    "  # backward pass：梯度清零 + 反向传播\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # 参数更新（带简单的学习率 schedule）\n",
    "  # lr = lrs[i]               # 如果想用前面准备的学习率表，可以改用这行\n",
    "  lr = 0.1 if i < 100000 else 0.01  # 前 10 万步用 0.1，后 10 万步用 0.01\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad          # 梯度下降更新参数\n",
    "\n",
    "  # 记录训练过程的统计\n",
    "  # lri.append(lre[i])\n",
    "  stepi.append(i)\n",
    "  lossi.append(loss.log10().item()) # 用 log10(loss) 方便可视化范围\n",
    "\n",
    "# print(loss.item())  # 最后看一下最终 loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13f906d50>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 画出训练过程中 loss 的变化\n",
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1136, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在训练集上计算最终损失（使用所有训练样本）\n",
    "emb = C[Xtr]                               # (Ntr, 3, 10)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (Ntr, 200)\n",
    "logits = h @ W2 + b2                       # (Ntr, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)        # 训练集 loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1540, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在 dev 集上计算损失，用于评估泛化效果\n",
    "emb = C[Xdev]                              # (Ndev, 3, 10)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (Ndev, 200)\n",
    "logits = h @ W2 + b2                       # (Ndev, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)       # 验证集 loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化嵌入矩阵 C 的前两个维度（只是一个 low-dim 可视化）\n",
    "# 看不同字符在嵌入空间中的分布\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(C[:, 0].data, C[:, 1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i, 0].item(), C[i, 1].item(), itos[i],\n",
    "             ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是关于 train / dev / test 划分的小注释：\n",
    "# training split, dev/validation split, test split\n",
    "# 80%, 10%, 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = [0] * block_size\n",
    "C[torch.tensor([context])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahxa.\n",
      "jehmarik.\n",
      "mis.\n",
      "reh.\n",
      "caspanden.\n",
      "jazhett.\n",
      "deliah.\n",
      "jarqui.\n",
      "ner.\n",
      "kia.\n",
      "chaiir.\n",
      "kaleigh.\n",
      "ham.\n",
      "jord.\n",
      "quint.\n",
      "shoisea.\n",
      "jaddi.\n",
      "wazelo.\n",
      "dearyxia.\n",
      "kael.\n"
     ]
    }
   ],
   "source": [
    "# 从训练好的模型中采样生成名字\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)  # 新的随机种子，防止和训练阶段冲突\n",
    "\n",
    "for _ in range(20):  # 生成 20 个名字\n",
    "    \n",
    "    out = []                                   # 用来存储生成的字符 id 序列\n",
    "    context = [0] * block_size                 # 初始上下文全 0，相当于 '...'\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])]         # (1, block_size, 10)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)# (1, 200)\n",
    "      logits = h @ W2 + b2                     # (1, 27)\n",
    "      probs = F.softmax(logits, dim=1)         # softmax -> 概率分布\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "                                               # 按概率分布随机采样一个字符 id\n",
    "      context = context[1:] + [ix]             # 更新上下文（滑动窗口）\n",
    "      out.append(ix)                           # 记录当前采样的字符 id\n",
    "      if ix == 0:                              # 如果采到 0（即 '.'），结束这个名字\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))       # 把 id 序列转成字符串并打印出来\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
